{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23FovOPuGf7j"
   },
   "source": [
    "Sumit Parwal   INF 552 Q1 5174593050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "HvAerJk2Gf7n",
    "outputId": "9c23e67f-181a-493a-8422-4cb024005c15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import one_hot\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io,os\n",
    "import re\n",
    "\n",
    "from __future__ import absolute_import, division, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 800
    },
    "colab_type": "code",
    "id": "nI7pYzMU6SQF",
    "outputId": "61fbac01-e943-4348-b095-1110bf497394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.16.4)\n",
      "Collecting tf_nightly\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/ff/5f0ae194e63c0de4501fbd894150e95e4f79df6c271c47fa26ae8fbd6224/tf_nightly-1.15.0.dev20190723-cp36-cp36m-manylinux1_x86_64.whl (102.1MB)\n",
      "\u001b[K     |████████████████████████████████| 102.1MB 1.2MB/s \n",
      "\u001b[?25hCollecting tf-estimator-nightly (from tf_nightly)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/8e/ed741122bad2bae27656edf8f8d0d748456968419201a527302d185c31f5/tf_estimator_nightly-1.14.0.dev2019072301-py2.py3-none-any.whl (501kB)\n",
      "\u001b[K     |████████████████████████████████| 501kB 45.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (0.8.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (3.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.11.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (0.7.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (0.2.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (0.33.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.15.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.16.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (0.1.7)\n",
      "Collecting opt-einsum>=2.3.2 (from tf_nightly)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 27.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf_nightly) (1.1.0)\n",
      "Collecting tb-nightly<1.16.0a0,>=1.15.0a0 (from tf_nightly)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/5e/047c04b620bff5c65703acba90d0bf31253fe2444cb41e88a3eb5277991c/tb_nightly-1.15.0a20190723-py3-none-any.whl (4.1MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1MB 40.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf_nightly) (41.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf_nightly) (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf_nightly) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf_nightly) (0.15.5)\n",
      "Building wheels for collected packages: opt-einsum\n",
      "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
      "Successfully built opt-einsum\n",
      "Installing collected packages: tf-estimator-nightly, opt-einsum, tb-nightly, tf-nightly\n",
      "Successfully installed opt-einsum-2.3.2 tb-nightly-1.15.0a20190723 tf-estimator-nightly-1.14.0.dev2019072301 tf-nightly-1.15.0.dev20190723\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "tensorboard",
         "tensorflow",
         "tensorflow_estimator"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install h5py pyyaml\n",
    "!pip install tf_nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "UexGATlOADQ5",
    "outputId": "378e52e3-6669-4705-d1ff-6494f34dbfa3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-8e0eccba-b0f6-4e0d-8bc5-0a5e03a54638\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-8e0eccba-b0f6-4e0d-8bc5-0a5e03a54638\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving AIIMAT.txt to AIIMAT.txt\n",
      "Saving MLOE.txt to MLOE.txt\n",
      "Saving OKEWFSMP.txt to OKEWFSMP.txt\n",
      "Saving TAM.txt to TAM.txt\n",
      "Saving TAMatter.txt to TAMatter.txt\n",
      "Saving THWP.txt to THWP.txt\n",
      "Saving TPP.txt to TPP.txt\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tdajkXgtAWqv"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "text0 =  io.BytesIO(uploaded['AIIMAT.txt']).read().lower().decode('ISO-8859-1')\n",
    "text1 =  io.BytesIO(uploaded['MLOE.txt']).read().lower().decode('ISO-8859-1')\n",
    "text2 =  io.BytesIO(uploaded['OKEWFSMP.txt']).read().lower().decode('ISO-8859-1')\n",
    "text3 =  io.BytesIO(uploaded['TAM.txt']).read().lower().decode('ISO-8859-1')\n",
    "text4 =  io.BytesIO(uploaded['TAMatter.txt']).read().lower().decode('ISO-8859-1')\n",
    "text5 =  io.BytesIO(uploaded['THWP.txt']).read().lower().decode('ISO-8859-1')\n",
    "text6 =  io.BytesIO(uploaded['TPP.txt']).read().lower().decode('ISO-8859-1')\n",
    "\n",
    "# len(text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2haZE10_mW2"
   },
   "source": [
    "c)i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFfBm6edQuaB"
   },
   "outputs": [],
   "source": [
    "text=text1+text2+text3+text5+text6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "aJpPVrUg_beg",
    "outputId": "a46ae5c8-71dd-4eba-ff1b-de2e834a696e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3640664"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83-uMC_eGf9b"
   },
   "outputs": [],
   "source": [
    "text = re.sub('[^a-zA-Z\\.\\,]',' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HCtkeNGGgC8"
   },
   "outputs": [],
   "source": [
    "text=re.sub( '\\s+', ' ', text ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NuEW670qGgDg"
   },
   "outputs": [],
   "source": [
    "alpharray=list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vurpx_6_GgEm"
   },
   "outputs": [],
   "source": [
    "array_ascii=[ord(c) for c in alpharray]\n",
    "array_scaled=[((c-32)/90) for c in array_ascii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4ySs-rOGgGO"
   },
   "outputs": [],
   "source": [
    "to_char=dict((c,chr(int((c*90)+32))) for i, c in enumerate(set(array_scaled)))\n",
    "to_ascii=dict((chr(int((c*90)+32)),c) for i, c in enumerate(set(array_scaled)))\n",
    "del alpharray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "id": "s9ueILLzGM2r",
    "outputId": "d3dcb954-4ad6-49c4-abbe-e7e42795a993",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0.0,\n",
       " ',': 0.13333333333333333,\n",
       " '.': 0.15555555555555556,\n",
       " 'a': 0.7222222222222222,\n",
       " 'b': 0.7333333333333333,\n",
       " 'c': 0.7444444444444445,\n",
       " 'd': 0.7555555555555555,\n",
       " 'e': 0.7666666666666667,\n",
       " 'f': 0.7777777777777778,\n",
       " 'g': 0.7888888888888889,\n",
       " 'h': 0.8,\n",
       " 'i': 0.8111111111111111,\n",
       " 'j': 0.8222222222222222,\n",
       " 'k': 0.8333333333333334,\n",
       " 'l': 0.8444444444444444,\n",
       " 'm': 0.8555555555555555,\n",
       " 'n': 0.8666666666666667,\n",
       " 'o': 0.8777777777777778,\n",
       " 'p': 0.8888888888888888,\n",
       " 'q': 0.9,\n",
       " 'r': 0.9111111111111111,\n",
       " 's': 0.9222222222222223,\n",
       " 't': 0.9333333333333333,\n",
       " 'u': 0.9444444444444444,\n",
       " 'v': 0.9555555555555556,\n",
       " 'w': 0.9666666666666667,\n",
       " 'x': 0.9777777777777777,\n",
       " 'y': 0.9888888888888889,\n",
       " 'z': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_ascii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzAmQgcrGgTN"
   },
   "outputs": [],
   "source": [
    "to_enc={}\n",
    "inc=0\n",
    "for c in sorted(to_ascii.values()):\n",
    "    to_enc.update({c:inc})\n",
    "    inc+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgaJHLXDGgUI"
   },
   "outputs": [],
   "source": [
    "decode={}\n",
    "inc=0\n",
    "for c in sorted(to_ascii.values()):\n",
    "    decode.update({inc:c})\n",
    "    inc+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2L7uh8kGgbW"
   },
   "outputs": [],
   "source": [
    "chars=set(array_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JryZW5NTGgkA"
   },
   "source": [
    "\n",
    "iii)Choose a window size, e.g., W = 100.\n",
    "\n",
    "iv) Inputs to the network will be the first W􀀀1 = 99 characters of each sequence,\n",
    "and the output of the network will be the Wth character of the sequence.\n",
    "Basically, we are training the network to predict each character using the 99\n",
    "characters that precede it. Slide the window in strides of S = 1 on the text.\n",
    "For example, if W = 5 and S = 1 and we want to train the network with the\n",
    "sequence ABRACADABRA, The first input to the network will be ABRA\n",
    "and the corresponding output will be C. The second input will be BRAC and\n",
    "the second output will be A, etc.\n",
    "\n",
    "v) Note that the output has to be encoded using a one-hot encoding scheme with\n",
    "N = 256 (or less) elements. This means that the network reads integers, but\n",
    "outputs a vector of N = 256 (or less) elements.\n",
    "\n",
    "vi) Use a single hidden layer for the LSTM with N = 256 (or less) memory units.\n",
    "\n",
    "vii) Use a Softmax output layer to yield a probability prediction for each of the\n",
    "characters between 0 and 1. This is actually a character classi\f",
    "cation problem\n",
    "with N classes. Choose log loss (cross entropy) as the objective function for\n",
    "the network (research what it means).3\n",
    "\n",
    "viii)We do not use a test dataset. We are using the whole training dataset to\n",
    "learn the probability of each character in a sequence. We are not seeking for\n",
    "a very accurate model. Instead we are interested in a generalization of the\n",
    "dataset that can mimic the gist of the text.\n",
    "\n",
    "ix.) Choose a reasonable number of epochs4 for training, considering your compu-\n",
    "tational power (e.g., 30, although the network will need more epochs to yield\n",
    "a better model).\n",
    "\n",
    "x.) Use model checkpointing to keep the network weights to determine each time\n",
    "an improvement in loss is observed at the end of the epoch. Find the best set\n",
    "of weights in terms of loss.\n",
    "\n",
    "xi) Use the network with the best weights to generate 1000 characters, using the\n",
    "following text as initialization of the network:\n",
    "There are those who take mental phenomena naively, just as they\n",
    "would physical phenomena. This school of psychologists tends not to\n",
    "emphasize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "GG5L27ODGgb9",
    "outputId": "2df79f27-8e20-4415-e9cb-2f589a33ebda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 3510278\n"
     ]
    }
   ],
   "source": [
    "N = 99\n",
    "\n",
    "def nSentences(fullarray,maxlen):\n",
    "    step = 1\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(fullarray) - maxlen, step):\n",
    "        sentences.append(fullarray[i: i + maxlen])\n",
    "        next_chars.append(fullarray[i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "    return sentences,next_chars\n",
    "sentences,next_char=nSentences(array_scaled,N)\n",
    "del array_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "C1k_1b0CGgcq",
    "outputId": "53ec38cf-e5e8-4524-8ace-90b0cb5bb6cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produce X and Y to train\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3510278, 99, 29)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Produce X and Y to train')\n",
    "x = np.zeros((len(sentences), N, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "     for k , char in enumerate(sentence):\n",
    "        x[i, k , toenc[char]] = 1\n",
    "        y[i, toenc[next_char[i]]] = 1\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDDqpACfeATC"
   },
   "outputs": [],
   "source": [
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "colab_type": "code",
    "id": "HvPuq42VGghy",
    "outputId": "c36ddcdb-da88-487b-e246-3e522f20232f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0723 17:24:39.560538 140415099955072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0723 17:24:39.607721 140415099955072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0723 17:24:39.614494 140415099955072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0723 17:24:39.948050 140415099955072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0723 17:24:39.955442 140415099955072 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               292864    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 29)                7453      \n",
      "=================================================================\n",
      "Total params: 300,317\n",
      "Trainable params: 300,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(N, len(chars))))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(len(chars),activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLV-h4mZ_bfT"
   },
   "source": [
    "\n",
    "\n",
    "x. Use model checkpointing to keep the network weights to determine each time an improvement in loss is observed at the end of the epoch. Find the best set of weights in terms of loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vm66SbaIGgis"
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "checkpoint_path =\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "IQsXR0WAGgiw",
    "outputId": "aefab500-a319-4a86-d1e2-90be08d3ecc1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 17:24:39.995512 140415099955072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0723 17:24:40.018582 140415099955072 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xMxEsmKqGgi1",
    "outputId": "b6394618-27b3-46b4-c6d2-f09d58b424ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 17:24:40.144826 140415099955072 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3510278/3510278 [==============================] - 440s 125us/step - loss: 2.2435\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.24349, saving model to weights-improvement-01-2.2435.hdf5\n",
      "Epoch 2/50\n",
      "3510278/3510278 [==============================] - 444s 127us/step - loss: 1.7685\n",
      "\n",
      "Epoch 00002: loss improved from 2.24349 to 1.76851, saving model to weights-improvement-02-1.7685.hdf5\n",
      "Epoch 3/50\n",
      "3510278/3510278 [==============================] - 445s 127us/step - loss: 1.5822\n",
      "\n",
      "Epoch 00003: loss improved from 1.76851 to 1.58221, saving model to weights-improvement-03-1.5822.hdf5\n",
      "Epoch 4/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.4766\n",
      "\n",
      "Epoch 00004: loss improved from 1.58221 to 1.47661, saving model to weights-improvement-04-1.4766.hdf5\n",
      "Epoch 5/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.4091\n",
      "\n",
      "Epoch 00005: loss improved from 1.47661 to 1.40910, saving model to weights-improvement-05-1.4091.hdf5\n",
      "Epoch 6/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.3620\n",
      "\n",
      "Epoch 00006: loss improved from 1.40910 to 1.36198, saving model to weights-improvement-06-1.3620.hdf5\n",
      "Epoch 7/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.3271\n",
      "\n",
      "Epoch 00007: loss improved from 1.36198 to 1.32705, saving model to weights-improvement-07-1.3271.hdf5\n",
      "Epoch 8/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.3003\n",
      "\n",
      "Epoch 00008: loss improved from 1.32705 to 1.30030, saving model to weights-improvement-08-1.3003.hdf5\n",
      "Epoch 9/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.2789\n",
      "\n",
      "Epoch 00009: loss improved from 1.30030 to 1.27885, saving model to weights-improvement-09-1.2789.hdf5\n",
      "Epoch 10/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.2608\n",
      "\n",
      "Epoch 00010: loss improved from 1.27885 to 1.26077, saving model to weights-improvement-10-1.2608.hdf5\n",
      "Epoch 11/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.2462\n",
      "\n",
      "Epoch 00011: loss improved from 1.26077 to 1.24623, saving model to weights-improvement-11-1.2462.hdf5\n",
      "Epoch 12/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.2336\n",
      "\n",
      "Epoch 00012: loss improved from 1.24623 to 1.23358, saving model to weights-improvement-12-1.2336.hdf5\n",
      "Epoch 13/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.2223\n",
      "\n",
      "Epoch 00013: loss improved from 1.23358 to 1.22230, saving model to weights-improvement-13-1.2223.hdf5\n",
      "Epoch 14/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.2123\n",
      "\n",
      "Epoch 00014: loss improved from 1.22230 to 1.21229, saving model to weights-improvement-14-1.2123.hdf5\n",
      "Epoch 15/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.2037\n",
      "\n",
      "Epoch 00015: loss improved from 1.21229 to 1.20374, saving model to weights-improvement-15-1.2037.hdf5\n",
      "Epoch 16/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1959\n",
      "\n",
      "Epoch 00016: loss improved from 1.20374 to 1.19590, saving model to weights-improvement-16-1.1959.hdf5\n",
      "Epoch 17/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.1888\n",
      "\n",
      "Epoch 00017: loss improved from 1.19590 to 1.18879, saving model to weights-improvement-17-1.1888.hdf5\n",
      "Epoch 18/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1823\n",
      "\n",
      "Epoch 00018: loss improved from 1.18879 to 1.18233, saving model to weights-improvement-18-1.1823.hdf5\n",
      "Epoch 19/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1765\n",
      "\n",
      "Epoch 00019: loss improved from 1.18233 to 1.17655, saving model to weights-improvement-19-1.1765.hdf5\n",
      "Epoch 20/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1715\n",
      "\n",
      "Epoch 00020: loss improved from 1.17655 to 1.17146, saving model to weights-improvement-20-1.1715.hdf5\n",
      "Epoch 21/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1667\n",
      "\n",
      "Epoch 00021: loss improved from 1.17146 to 1.16673, saving model to weights-improvement-21-1.1667.hdf5\n",
      "Epoch 22/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1621\n",
      "\n",
      "Epoch 00022: loss improved from 1.16673 to 1.16209, saving model to weights-improvement-22-1.1621.hdf5\n",
      "Epoch 23/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1582\n",
      "\n",
      "Epoch 00023: loss improved from 1.16209 to 1.15818, saving model to weights-improvement-23-1.1582.hdf5\n",
      "Epoch 24/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1543\n",
      "\n",
      "Epoch 00024: loss improved from 1.15818 to 1.15428, saving model to weights-improvement-24-1.1543.hdf5\n",
      "Epoch 25/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1507\n",
      "\n",
      "Epoch 00025: loss improved from 1.15428 to 1.15067, saving model to weights-improvement-25-1.1507.hdf5\n",
      "Epoch 26/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1471\n",
      "\n",
      "Epoch 00026: loss improved from 1.15067 to 1.14707, saving model to weights-improvement-26-1.1471.hdf5\n",
      "Epoch 27/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1437\n",
      "\n",
      "Epoch 00027: loss improved from 1.14707 to 1.14367, saving model to weights-improvement-27-1.1437.hdf5\n",
      "Epoch 28/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.1410\n",
      "\n",
      "Epoch 00028: loss improved from 1.14367 to 1.14104, saving model to weights-improvement-28-1.1410.hdf5\n",
      "Epoch 29/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1378\n",
      "\n",
      "Epoch 00029: loss improved from 1.14104 to 1.13784, saving model to weights-improvement-29-1.1378.hdf5\n",
      "Epoch 30/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1355\n",
      "\n",
      "Epoch 00030: loss improved from 1.13784 to 1.13551, saving model to weights-improvement-30-1.1355.hdf5\n",
      "Epoch 31/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1329\n",
      "\n",
      "Epoch 00031: loss improved from 1.13551 to 1.13289, saving model to weights-improvement-31-1.1329.hdf5\n",
      "Epoch 32/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1307\n",
      "\n",
      "Epoch 00032: loss improved from 1.13289 to 1.13071, saving model to weights-improvement-32-1.1307.hdf5\n",
      "Epoch 33/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1282\n",
      "\n",
      "Epoch 00033: loss improved from 1.13071 to 1.12825, saving model to weights-improvement-33-1.1282.hdf5\n",
      "Epoch 34/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1263\n",
      "\n",
      "Epoch 00034: loss improved from 1.12825 to 1.12629, saving model to weights-improvement-34-1.1263.hdf5\n",
      "Epoch 35/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.1244\n",
      "\n",
      "Epoch 00035: loss improved from 1.12629 to 1.12436, saving model to weights-improvement-35-1.1244.hdf5\n",
      "Epoch 36/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1223\n",
      "\n",
      "Epoch 00036: loss improved from 1.12436 to 1.12229, saving model to weights-improvement-36-1.1223.hdf5\n",
      "Epoch 37/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1201\n",
      "\n",
      "Epoch 00037: loss improved from 1.12229 to 1.12012, saving model to weights-improvement-37-1.1201.hdf5\n",
      "Epoch 38/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1187\n",
      "\n",
      "Epoch 00038: loss improved from 1.12012 to 1.11874, saving model to weights-improvement-38-1.1187.hdf5\n",
      "Epoch 39/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1169\n",
      "\n",
      "Epoch 00039: loss improved from 1.11874 to 1.11694, saving model to weights-improvement-39-1.1169.hdf5\n",
      "Epoch 40/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1153\n",
      "\n",
      "Epoch 00040: loss improved from 1.11694 to 1.11527, saving model to weights-improvement-40-1.1153.hdf5\n",
      "Epoch 41/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1136\n",
      "\n",
      "Epoch 00041: loss improved from 1.11527 to 1.11358, saving model to weights-improvement-41-1.1136.hdf5\n",
      "Epoch 42/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1121\n",
      "\n",
      "Epoch 00042: loss improved from 1.11358 to 1.11215, saving model to weights-improvement-42-1.1121.hdf5\n",
      "Epoch 43/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1112\n",
      "\n",
      "Epoch 00043: loss improved from 1.11215 to 1.11122, saving model to weights-improvement-43-1.1112.hdf5\n",
      "Epoch 44/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1096\n",
      "\n",
      "Epoch 00044: loss improved from 1.11122 to 1.10957, saving model to weights-improvement-44-1.1096.hdf5\n",
      "Epoch 45/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1083\n",
      "\n",
      "Epoch 00045: loss improved from 1.10957 to 1.10830, saving model to weights-improvement-45-1.1083.hdf5\n",
      "Epoch 46/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1067\n",
      "\n",
      "Epoch 00046: loss improved from 1.10830 to 1.10671, saving model to weights-improvement-46-1.1067.hdf5\n",
      "Epoch 47/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1055\n",
      "\n",
      "Epoch 00047: loss improved from 1.10671 to 1.10554, saving model to weights-improvement-47-1.1055.hdf5\n",
      "Epoch 48/50\n",
      "3510278/3510278 [==============================] - 446s 127us/step - loss: 1.1044\n",
      "\n",
      "Epoch 00048: loss improved from 1.10554 to 1.10438, saving model to weights-improvement-48-1.1044.hdf5\n",
      "Epoch 49/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1035\n",
      "\n",
      "Epoch 00049: loss improved from 1.10438 to 1.10350, saving model to weights-improvement-49-1.1035.hdf5\n",
      "Epoch 50/50\n",
      "3510278/3510278 [==============================] - 447s 127us/step - loss: 1.1025\n",
      "\n",
      "Epoch 00050: loss improved from 1.10350 to 1.10252, saving model to weights-improvement-50-1.1025.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb49aa040b8>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y, epochs=50, batch_size=5000,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4htmaxUwGgjJ",
    "outputId": "d7bc15a9-f5a8-4274-b983-a3b8f10af9e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques=\"There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.\"\n",
    "ques=ques.lower()\n",
    "ques=ques[-N:]\n",
    "len(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4XP4qXwGgjS"
   },
   "outputs": [],
   "source": [
    "test_array_ascii=[ord(c) for c in ques]\n",
    "test_scaled_array=[((c-32)/90) for c in test_array_ascii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ndy9gIHiGgjc"
   },
   "outputs": [],
   "source": [
    "test_sentences=[test_scaled_array]\n",
    "x_t = np.zeros((len(test_sentences), N, len(chars)), dtype=np.bool)\n",
    "for i, tsentence in enumerate(test_sentences):\n",
    "     for k , char in enumerate(tsentence):\n",
    "        x_t[i, k , toenc[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-_BQ0elGgjk"
   },
   "outputs": [],
   "source": [
    "x_t.shape\n",
    "prediction = model.predict(x_t, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "7tzXFg5G1454",
    "outputId": "ba0c26bf-6ba4-43ac-f4b1-572e5908c1fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t as they would physical phenomena. this school of psychologists tends not to emphasize the object.\n",
      "Generated text :\n",
      " the sense data is the same that it is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that the"
     ]
    }
   ],
   "source": [
    "print(ques)\n",
    "print(\"Generated text :\")\n",
    "for i in range(1000):\n",
    "    prediction = model.predict(x_t, verbose=0)\n",
    "    print(tochar[decode[np.argmax(prediction)]],end=\"\")\n",
    "    x_t[0][:-1]=x_t[0][1:]\n",
    "    k=np.zeros(len(chars), dtype=np.bool)\n",
    "    k[np.argmax(prediction)]=1\n",
    "    x_t[0][-1]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "dxUQL0iYGgj1",
    "outputId": "367a5e5f-e548-4407-97d4-74ed27467dcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t as they would physical phenomena. this school of psychologists tends not to emphasize the object.\n",
      "Generated text :\n",
      "re is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same "
     ]
    }
   ],
   "source": [
    "print(ques)\n",
    "print(\"Generated text :\")\n",
    "for i in range(1000):\n",
    "    prediction = model.predict(x_t, verbose=0)\n",
    "    print(tochar[decode[np.argmax(prediction)]],end=\"\")\n",
    "    x_t[0][:-1]=x_t[0][1:]\n",
    "    k=np.zeros(len(chars), dtype=np.bool)\n",
    "    k[np.argmax(prediction)]=1\n",
    "    x_t[0][-1]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "6WdDz-YZu13W",
    "outputId": "44f84737-cc63-40b1-a117-52d96721cba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t as they would physical phenomena. this school of psychologists tends not to emphasize the object.\n",
      "Generated text :\n",
      "that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same "
     ]
    }
   ],
   "source": [
    "print(ques)\n",
    "print(\"Generated text :\")\n",
    "for i in range(400):\n",
    "    prediction = model.predict(x_t, verbose=0)\n",
    "    print(tochar[decode[np.argmax(prediction)]],end=\"\")\n",
    "    x_t[0][:-1]=x_t[0][1:]\n",
    "    k=np.zeros(len(chars), dtype=np.bool)\n",
    "    k[np.argmax(prediction)]=1\n",
    "    x_t[0][-1]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "9ITsm2H-u2D2",
    "outputId": "b77bec3c-8513-4418-a0b7-c328dd31843a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t as they would physical phenomena. this school of psychologists tends not to emphasize the object.\n",
      "Generated text :\n",
      "that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is t"
     ]
    }
   ],
   "source": [
    "print(ques)\n",
    "print(\"Generated text :\")\n",
    "for i in range(1000):\n",
    "    prediction = model.predict(x_t, verbose=0)\n",
    "    print(tochar[decode[np.argmax(prediction)]],end=\"\")\n",
    "    x_t[0][:-1]=x_t[0][1:]\n",
    "    k=np.zeros(len(chars), dtype=np.bool)\n",
    "    k[np.argmax(prediction)]=1\n",
    "    x_t[0][-1]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "eSARl10bu2L6",
    "outputId": "269735ee-5547-475a-8e88-f71a59079999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t as they would physical phenomena. this school of psychologists tends not to emphasize the object.\n",
      "Generated text :\n",
      "he same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data which is the same that it is the same that there is no such thing as the sense data wh"
     ]
    }
   ],
   "source": [
    "print(ques)\n",
    "print(\"Generated text :\")\n",
    "for i in range(1000):\n",
    "    prediction = model.predict(x_t, verbose=0)\n",
    "    print(tochar[decode[np.argmax(prediction)]],end=\"\")\n",
    "    x_t[0][:-1]=x_t[0][1:]\n",
    "    k=np.zeros(len(chars), dtype=np.bool)\n",
    "    k[np.argmax(prediction)]=1\n",
    "    x_t[0][-1]=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "colab_type": "code",
    "id": "yukazEtHfv59",
    "outputId": "6eac818c-63f6-498f-c3e0-a42b62db0321"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIIMAT.txt\t\t\t    weights-improvement-22-1.1621.hdf5\n",
      "MLOE.txt\t\t\t    weights-improvement-23-1.1582.hdf5\n",
      "OKEWFSMP.txt\t\t\t    weights-improvement-24-1.1543.hdf5\n",
      "sample_data\t\t\t    weights-improvement-25-1.1507.hdf5\n",
      "TAMatter.txt\t\t\t    weights-improvement-26-1.1471.hdf5\n",
      "TAM.txt\t\t\t\t    weights-improvement-27-1.1437.hdf5\n",
      "THWP.txt\t\t\t    weights-improvement-28-1.1410.hdf5\n",
      "TPP.txt\t\t\t\t    weights-improvement-29-1.1378.hdf5\n",
      "weights-improvement-01-2.2435.hdf5  weights-improvement-30-1.1355.hdf5\n",
      "weights-improvement-02-1.7685.hdf5  weights-improvement-31-1.1329.hdf5\n",
      "weights-improvement-03-1.5822.hdf5  weights-improvement-32-1.1307.hdf5\n",
      "weights-improvement-04-1.4766.hdf5  weights-improvement-33-1.1282.hdf5\n",
      "weights-improvement-05-1.4091.hdf5  weights-improvement-34-1.1263.hdf5\n",
      "weights-improvement-06-1.3620.hdf5  weights-improvement-35-1.1244.hdf5\n",
      "weights-improvement-07-1.3271.hdf5  weights-improvement-36-1.1223.hdf5\n",
      "weights-improvement-08-1.3003.hdf5  weights-improvement-37-1.1201.hdf5\n",
      "weights-improvement-09-1.2789.hdf5  weights-improvement-38-1.1187.hdf5\n",
      "weights-improvement-10-1.2608.hdf5  weights-improvement-39-1.1169.hdf5\n",
      "weights-improvement-11-1.2462.hdf5  weights-improvement-40-1.1153.hdf5\n",
      "weights-improvement-12-1.2336.hdf5  weights-improvement-41-1.1136.hdf5\n",
      "weights-improvement-13-1.2223.hdf5  weights-improvement-42-1.1121.hdf5\n",
      "weights-improvement-14-1.2123.hdf5  weights-improvement-43-1.1112.hdf5\n",
      "weights-improvement-15-1.2037.hdf5  weights-improvement-44-1.1096.hdf5\n",
      "weights-improvement-16-1.1959.hdf5  weights-improvement-45-1.1083.hdf5\n",
      "weights-improvement-17-1.1888.hdf5  weights-improvement-46-1.1067.hdf5\n",
      "weights-improvement-18-1.1823.hdf5  weights-improvement-47-1.1055.hdf5\n",
      "weights-improvement-19-1.1765.hdf5  weights-improvement-48-1.1044.hdf5\n",
      "weights-improvement-20-1.1715.hdf5  weights-improvement-49-1.1035.hdf5\n",
      "weights-improvement-21-1.1667.hdf5  weights-improvement-50-1.1025.hdf5\n"
     ]
    }
   ],
   "source": [
    "! ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgrJHEVAL4Om"
   },
   "outputs": [],
   "source": [
    "#Load backed up model\n",
    "filename = \"weights-improvement-21-1.1667.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "sumit_parwal_hw_7_q1 (1).ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
